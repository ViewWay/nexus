# Nexus Framework - Benchmarking Guide
# Nexusæ¡†æ¶ - æ€§èƒ½åŸºå‡†æµ‹è¯•æŒ‡å—

**Version**: 0.1.0-alpha
**Date**: 2026-01-24
**Status**: Phase 1 Completed âœ…, Phase 2 Benchmarks Completed âœ…
**çŠ¶æ€**: ç¬¬1é˜¶æ®µå·²å®Œæˆ âœ…ï¼Œç¬¬2é˜¶æ®µåŸºå‡†æµ‹è¯•å·²å®Œæˆ âœ…

---

## Table of Contents / ç›®å½•

1. [Overview / æ¦‚è§ˆ](#1-overview-æ¦‚è§ˆ)
2. [Benchmarking Strategy / åŸºå‡†æµ‹è¯•ç­–ç•¥](#2-benchmarking-strategy-åŸºå‡†æµ‹è¯•ç­–ç•¥)
3. [Phase 1: Runtime Benchmarks / ç¬¬1é˜¶æ®µï¼šè¿è¡Œæ—¶åŸºå‡†æµ‹è¯•](#3-phase-1-runtime-benchmarks-ç¬¬1é˜¶æ®µè¿è¡Œæ—¶åŸºå‡†æµ‹è¯•)
4. [Phase 2: HTTP Benchmarks / ç¬¬2é˜¶æ®µï¼šHTTPåŸºå‡†æµ‹è¯•](#4-phase-2-http-benchmarks-ç¬¬2é˜¶æ®µhttpåŸºå‡†æµ‹è¯•)
5. [Tools & Environment / å·¥å…·ä¸ç¯å¢ƒ](#5-tools--environment-å·¥å…·ä¸ç¯å¢ƒ)
6. [Running Benchmarks / è¿è¡ŒåŸºå‡†æµ‹è¯•](#6-running-benchmarks-è¿è¡ŒåŸºå‡†æµ‹è¯•)
7. [Performance Regression Detection / æ€§èƒ½å›å½’æ£€æµ‹](#7-performance-regression-detection-æ€§èƒ½å›å½’æ£€æµ‹)
8. [Benchmark Results Archive / åŸºå‡†æµ‹è¯•ç»“æœå½’æ¡£](#8-benchmark-results-archive-åŸºå‡†æµ‹è¯•ç»“æœå½’æ¡£)

---

## 1. Overview / æ¦‚è§ˆ

### 1.1 Purpose / ç›®çš„

This document provides comprehensive guidelines for benchmarking the Nexus framework to:

æœ¬æ–‡æ¡£æä¾›äº†å¯¹Nexusæ¡†æ¶è¿›è¡ŒåŸºå‡†æµ‹è¯•çš„å…¨é¢æŒ‡å—ï¼Œä»¥ä¾¿ï¼š

- **Validate performance goals** / **éªŒè¯æ€§èƒ½ç›®æ ‡** - Ensure Nexus meets target QPS, latency, and memory usage
- **Compare with existing solutions** / **ä¸ç°æœ‰è§£å†³æ–¹æ¡ˆæ¯”è¾ƒ** - Benchmark against Tokio, Actix Web, Axum
- **Detect regressions** / **æ£€æµ‹æ€§èƒ½å›å½’** - Identify performance degradation across commits
- **Guide optimizations** / **æŒ‡å¯¼ä¼˜åŒ–** - Profile and identify bottlenecks

### 1.2 Performance Goals / æ€§èƒ½ç›®æ ‡

| Metric / æŒ‡æ ‡ | Target / ç›®æ ‡ | Baseline (Tokio) | Status / çŠ¶æ€ |
|--------------|---------------|------------------|--------------|
| **QPS** (simple echo) | 1M+ | ~800K | ğŸ“Š Phase 2 |
| **P99 latency** (no middleware) | < 1ms | ~1.5ms | ğŸ“Š Phase 2 |
| **P999 latency** | < 5ms | ~8ms | ğŸ“Š Phase 2 |
| **Memory** (idle) | < 10MB | ~16MB | ğŸ“Š Phase 2 |
| **Memory** (10K connections) | < 200MB | ~280MB | ğŸ“Š Phase 2 |
| **CPU efficiency** | 95%+ | ~85% | ğŸ“Š Phase 2 |
| **Startup time** | < 50ms | ~80ms | ğŸ“Š Phase 2 |
| **Syscalls per request** | < 3 | ~8 | ğŸ“Š Phase 2 |

### 1.3 Comparison Matrix / å¯¹æ¯”çŸ©é˜µ

| Framework | Runtime | I/O Backend | Scheduler | Our Goal |
|-----------|---------|-------------|-----------|----------|
| **Nexus** | Custom | io-uring (Linux) | Thread-per-core | **Baseline** |
| **Actix Web** | Tokio | epoll/kqueue | Work-stealing | +20% QPS |
| **Axum** | Tokio | epoll/kqueue | Work-stealing | +20% QPS |
| **Rocket** | Tokio | epoll/kqueue | Work-stealing | +30% QPS |
| **Hyper** | Tokio | epoll/kqueue | Work-stealing | +15% QPS |
| **Monoio** | Custom | io-uring | Thread-per-core | Comparable |

---

## 2. Benchmarking Strategy / åŸºå‡†æµ‹è¯•ç­–ç•¥

### 2.1 Benchmark Types / åŸºå‡†æµ‹è¯•ç±»å‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Benchmark Pyramid                        â”‚
â”‚                   åŸºå‡†æµ‹è¯•é‡‘å­—å¡”                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚                   â”‚  Integration    â”‚   10%               â”‚
â”‚                   â”‚  é›†æˆåŸºå‡†æµ‹è¯•    â”‚   - E2E scenarios   â”‚
â”‚                   â”‚  (TechEmpower)  â”‚   - Full stack      â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                                             â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚              â”‚     Component            â”‚   30%            â”‚
â”‚              â”‚     ç»„ä»¶åŸºå‡†æµ‹è¯•          â”‚   - HTTP parser  â”‚
â”‚              â”‚  (Criterion benches)    â”‚   - Router       â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   - Middleware  â”‚
â”‚                                                             â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚         â”‚          Micro                        â”‚   60%    â”‚
â”‚         â”‚          å¾®åŸºå‡†æµ‹è¯•                   â”‚   - spawnâ”‚
â”‚         â”‚      (inline benchmarks)             â”‚   - I/O  â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   - Timerâ”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2.1.1 Micro Benchmarks / å¾®åŸºå‡†æµ‹è¯•

**Purpose** / **ç›®çš„**: Measure individual operations in isolation.

**Tools** / **å·¥å…·**: `criterion` (Rust), custom harness

**Examples** / **ç¤ºä¾‹**:
- Task spawn latency / ä»»åŠ¡ç”Ÿæˆå»¶è¿Ÿ
- Channel send/recv throughput / é€šé“å‘é€/æ¥æ”¶ååé‡
- Timer wheel operations / æ—¶é—´è½®æ“ä½œ
- Driver submit/wait cycles / é©±åŠ¨æäº¤/ç­‰å¾…å‘¨æœŸ

#### 2.1.2 Component Benchmarks / ç»„ä»¶åŸºå‡†æµ‹è¯•

**Purpose** / **ç›®çš„**: Measure subsystem performance.

**Tools** / **å·¥å…·**: `criterion` (Rust)

**Examples** / **ç¤ºä¾‹**:
- HTTP request parsing / HTTPè¯·æ±‚è§£æ
- Router matching / è·¯ç”±åŒ¹é…
- Middleware chain execution / ä¸­é—´ä»¶é“¾æ‰§è¡Œ
- Response serialization / å“åº”åºåˆ—åŒ–

#### 2.1.3 Integration Benchmarks / é›†æˆåŸºå‡†æµ‹è¯•

**Purpose** / **ç›®çš„**: Measure real-world performance.

**Tools** / **å·¥å…·**: `wrk`, `hey`, `oha`, TechEmpower

**Examples** / **ç¤ºä¾‹**:
- Simple echo server / ç®€å•å›æ˜¾æœåŠ¡å™¨
- JSON API server / JSON APIæœåŠ¡å™¨
- Database query benchmark / æ•°æ®åº“æŸ¥è¯¢åŸºå‡†æµ‹è¯•
- Full-stack web application / å…¨æ ˆWebåº”ç”¨

---

## 3. Phase 1: Runtime Benchmarks / ç¬¬1é˜¶æ®µï¼šè¿è¡Œæ—¶åŸºå‡†æµ‹è¯•

### 3.1 Setup / è®¾ç½®

#### 3.1.1 Create Benchmark Directory / åˆ›å»ºåŸºå‡†æµ‹è¯•ç›®å½•

```bash
# Project structure / é¡¹ç›®ç»“æ„
nexus/
â”œâ”€â”€ benches/                    # Benchmark suite / åŸºå‡†æµ‹è¯•å¥—ä»¶
â”‚   â”œâ”€â”€ runtime_bench.rs        # Runtime core benchmarks
â”‚   â”œâ”€â”€ scheduler_bench.rs      # Scheduler benchmarks
â”‚   â”œâ”€â”€ io_bench.rs             # I/O driver benchmarks
â”‚   â”œâ”€â”€ timer_bench.rs          # Timer wheel benchmarks
â”‚   â”œâ”€â”€ channel_bench.rs        # Channel benchmarks
â”‚   â””â”€â”€ support/                # Shared utilities
â”‚       â””â”€â”€ mod.rs
â””â”€â”€ Cargo.toml                  # Add [[bench]] sections
```

#### 3.1.2 Cargo.toml Configuration / Cargo.tomlé…ç½®

```toml
# nexus-runtime/Cargo.toml

[dev-dependencies]
criterion = { version = "0.8", features = ["html_reports"] }
tokio = { version = "1.43", features = ["full"] }  # For comparison
rand = "0.9"

[[bench]]
name = "runtime"
harness = false

[[bench]]
name = "scheduler"
harness = false

[[bench]]
name = "io"
harness = false

[[bench]]
name = "timer"
harness = false

[[bench]]
name = "channel"
harness = false
```

### 3.2 Runtime Core Benchmarks / è¿è¡Œæ—¶æ ¸å¿ƒåŸºå‡†æµ‹è¯•

#### 3.2.1 Task Spawn Latency / ä»»åŠ¡ç”Ÿæˆå»¶è¿Ÿ

**Goal** / **ç›®æ ‡**: < 10Âµs per spawn

```rust
// benches/runtime_bench.rs
use criterion::{black_box, criterion_group, criterion_main, Criterion};
use nexus_runtime::{Runtime, spawn};

fn bench_spawn_latency(c: &mut Criterion) {
    let mut runtime = Runtime::new().unwrap();
    
    c.bench_function("spawn_latency", |b| {
        b.iter(|| {
            runtime.block_on(async {
                let handle = spawn(async {
                    black_box(42)
                });
                handle.wait().await.unwrap()
            }).unwrap();
        });
    });
}

criterion_group!(benches, bench_spawn_latency);
criterion_main!(benches);
```

#### 3.2.2 Channel Throughput / é€šé“ååé‡

**Goal** / **ç›®æ ‡**: > 10M ops/sec (bounded), > 5M ops/sec (unbounded)

```rust
// benches/channel_bench.rs
use criterion::{black_box, criterion_group, criterion_main, Criterion, Throughput};
use nexus_runtime::{Runtime, bounded, unbounded};

fn bench_bounded_channel(c: &mut Criterion) {
    let mut runtime = Runtime::new().unwrap();
    let mut group = c.benchmark_group("bounded_channel");
    
    for size in [1, 10, 100, 1000] {
        group.throughput(Throughput::Elements(size as u64));
        group.bench_with_input(format!("size_{}", size), &size, |b, &size| {
            b.iter(|| {
                runtime.block_on(async {
                    let (tx, rx) = bounded::<i32>(size);
                    
                    for i in 0..size {
                        tx.send(i).await.unwrap();
                    }
                    
                    for _ in 0..size {
                        black_box(rx.recv().await.unwrap());
                    }
                }).unwrap();
            });
        });
    }
    
    group.finish();
}

criterion_group!(benches, bench_bounded_channel);
criterion_main!(benches);
```

#### 3.2.3 Timer Wheel Operations / æ—¶é—´è½®æ“ä½œ

**Goal** / **ç›®æ ‡**: O(1) insertion, < 1Âµs overhead

```rust
// benches/timer_bench.rs
use criterion::{black_box, criterion_group, criterion_main, Criterion};
use nexus_runtime::{Runtime, sleep};
use std::time::Duration;

fn bench_timer_insert(c: &mut Criterion) {
    let mut runtime = Runtime::new().unwrap();
    
    c.bench_function("timer_insert", |b| {
        b.iter(|| {
            runtime.block_on(async {
                // Insert timer without waiting
                let _ = sleep(Duration::from_millis(black_box(100)));
            }).unwrap();
        });
    });
}

fn bench_timer_fire(c: &mut Criterion) {
    let mut runtime = Runtime::new().unwrap();
    
    c.bench_function("timer_fire", |b| {
        b.iter(|| {
            runtime.block_on(async {
                sleep(Duration::from_millis(1)).await;
            }).unwrap();
        });
    });
}

criterion_group!(benches, bench_timer_insert, bench_timer_fire);
criterion_main!(benches);
```

### 3.3 Scheduler Benchmarks / è°ƒåº¦å™¨åŸºå‡†æµ‹è¯•

#### 3.3.1 Task Queue Operations / ä»»åŠ¡é˜Ÿåˆ—æ“ä½œ

**Goal** / **ç›®æ ‡**: > 10M tasks/sec throughput

```rust
// benches/scheduler_bench.rs
use criterion::{black_box, criterion_group, criterion_main, Criterion};
use nexus_runtime::{Runtime, spawn};

fn bench_scheduler_throughput(c: &mut Criterion) {
    let mut runtime = Runtime::new().unwrap();
    
    c.bench_function("scheduler_throughput_1000_tasks", |b| {
        b.iter(|| {
            runtime.block_on(async {
                let mut handles = Vec::with_capacity(1000);
                
                for i in 0..1000 {
                    let handle = spawn(async move {
                        black_box(i * 2)
                    });
                    handles.push(handle);
                }
                
                for handle in handles {
                    black_box(handle.wait().await.unwrap());
                }
            }).unwrap();
        });
    });
}

criterion_group!(benches, bench_scheduler_throughput);
criterion_main!(benches);
```

### 3.4 I/O Driver Benchmarks / I/Oé©±åŠ¨åŸºå‡†æµ‹è¯•

#### 3.4.1 TCP Echo Throughput / TCPå›æ˜¾ååé‡

**Goal** / **ç›®æ ‡**: > 1M requests/sec (single connection)

```rust
// benches/io_bench.rs
use criterion::{black_box, criterion_group, criterion_main, Criterion, Throughput};
use nexus_runtime::{Runtime, io::{TcpListener, TcpStream}};

fn bench_tcp_echo(c: &mut Criterion) {
    let mut runtime = Runtime::new().unwrap();
    let mut group = c.benchmark_group("tcp_echo");
    group.throughput(Throughput::Bytes(1024));
    
    group.bench_function("1kb_payload", |b| {
        b.iter(|| {
            runtime.block_on(async {
                // TODO: Implement TCP echo benchmark
                // å¾…å®ç°TCPå›æ˜¾åŸºå‡†æµ‹è¯•
            }).unwrap();
        });
    });
    
    group.finish();
}

criterion_group!(benches, bench_tcp_echo);
criterion_main!(benches);
```

### 3.5 Comparison with Tokio / ä¸Tokioå¯¹æ¯”

```rust
// benches/runtime_bench.rs (continued)

fn bench_spawn_latency_tokio(c: &mut Criterion) {
    c.bench_function("spawn_latency_tokio", |b| {
        let runtime = tokio::runtime::Runtime::new().unwrap();
        
        b.iter(|| {
            runtime.block_on(async {
                let handle = tokio::spawn(async {
                    black_box(42)
                });
                handle.await.unwrap()
            });
        });
    });
}

criterion_group!(
    benches,
    bench_spawn_latency,        // Nexus
    bench_spawn_latency_tokio   // Tokio
);
criterion_main!(benches);
```

---

## 4. Phase 2: HTTP Benchmarks / ç¬¬2é˜¶æ®µï¼šHTTPåŸºå‡†æµ‹è¯•

> **Status** / **çŠ¶æ€**: âœ… Completed | Benchmark infrastructure set up with Criterion
> **çŠ¶æ€**ï¼šå·²å®Œæˆ | ä½¿ç”¨Criterionè®¾ç½®äº†åŸºå‡†æµ‹è¯•åŸºç¡€è®¾æ–½

> **Date** / **æ—¥æœŸ**: 2026-01-24

### 4.0 Benchmark Results Summary / åŸºå‡†æµ‹è¯•ç»“æœæ‘˜è¦

#### HTTP Server Benchmarks / HTTPæœåŠ¡å™¨åŸºå‡†æµ‹è¯•

| Benchmark | Time | Throughput | Notes |
|-----------|------|------------|-------|
| parse_simple_get | 170 ns | - | Simple GET request parsing |
| parse_get_with_headers | 215 ns | - | GET with multiple headers |
| parse_post_json | 617 ns | - | POST with JSON body |
| encode_response | 121 ns | - | Response serialization |
| encode_response_large | 403 ns | - | Large response (~10KB) |
| request_creation | 145 ns | - | Building HTTP request |
| response_creation | 5.1 ns | - | Building HTTP response |

**Throughput Results:**
- 64B POST: 124 MiB/s
- 256B POST: 488 MiB/s
- 1KB POST: 1.80 GiB/s
- 4KB POST: 6.80 GiB/s

#### Router Benchmarks / è·¯ç”±å™¨åŸºå‡†æµ‹è¯•

| Benchmark | Time | Notes |
|-----------|------|-------|
| route_registration | 10.4 Âµs | 100 routes registration |
| large_router_creation | 11.4 Âµs | Large router with params |
| static_route_registration | 418 ns | Static routes (5 routes) |
| param_route_registration | 589 ns | Routes with path params |
| request_creation | 69 ns | Request building |
| response_creation | 5.5 ns | Response building |

### 4.1 TechEmpower Benchmarks / TechEmpoweråŸºå‡†æµ‹è¯•

#### 4.1.1 Test Types / æµ‹è¯•ç±»å‹

| Test / æµ‹è¯• | Description / æè¿° | Nexus Target | Actix (Current) |
|------------|-------------------|--------------|-----------------|
| **JSON Serialization** | Return JSON response / è¿”å›JSONå“åº” | Top 20 | #8 |
| **Single Query** | Database SELECT / æ•°æ®åº“æŸ¥è¯¢ | Top 30 | #15 |
| **Multiple Queries** | N x SELECT / Næ¬¡æŸ¥è¯¢ | Top 30 | #18 |
| **Fortunes** | Template rendering / æ¨¡æ¿æ¸²æŸ“ | Top 40 | #25 |
| **Updates** | Database UPDATE / æ•°æ®åº“æ›´æ–° | Top 40 | #22 |
| **Plaintext** | Raw throughput / åŸå§‹ååé‡ | Top 10 | #5 |

#### 4.1.2 Setup Instructions / è®¾ç½®è¯´æ˜

```bash
# Clone TechEmpower Framework Benchmarks / å…‹éš†TechEmpoweræ¡†æ¶åŸºå‡†æµ‹è¯•
git clone https://github.com/TechEmpower/FrameworkBenchmarks.git
cd FrameworkBenchmarks

# Create Nexus benchmark / åˆ›å»ºNexusåŸºå‡†æµ‹è¯•
mkdir -p frameworks/Rust/nexus
cp -r frameworks/Rust/actix-web/* frameworks/Rust/nexus/

# Edit config.toml to add nexus / ç¼–è¾‘config.tomlæ·»åŠ nexus
# ...

# Run benchmark / è¿è¡ŒåŸºå‡†æµ‹è¯•
./tfb --mode verify --test nexus
./tfb --mode benchmark --test nexus
```

### 4.2 HTTP Parser Benchmarks / HTTPè§£æå™¨åŸºå‡†æµ‹è¯•

**Goal** / **ç›®æ ‡**: > 1GB/s parsing throughput

```rust
// benches/http_parser_bench.rs
use criterion::{black_box, criterion_group, criterion_main, Criterion, Throughput};

fn bench_http_request_parsing(c: &mut Criterion) {
    let mut group = c.benchmark_group("http_parser");
    
    let request = b"GET /hello HTTP/1.1\r\n\
                     Host: example.com\r\n\
                     User-Agent: benchmark\r\n\
                     Accept: */*\r\n\
                     \r\n";
    
    group.throughput(Throughput::Bytes(request.len() as u64));
    
    group.bench_function("parse_request", |b| {
        b.iter(|| {
            // Parse HTTP request / è§£æHTTPè¯·æ±‚
            // TODO: Implement parser benchmark
            black_box(request)
        });
    });
    
    group.finish();
}

criterion_group!(benches, bench_http_request_parsing);
criterion_main!(benches);
```

### 4.3 Router Benchmarks / è·¯ç”±å™¨åŸºå‡†æµ‹è¯•

**Goal** / **ç›®æ ‡**: < 100ns per route match

```rust
// benches/router_bench.rs
use criterion::{black_box, criterion_group, criterion_main, Criterion, BenchmarkId};
use nexus_router::Router;

fn bench_router_match(c: &mut Criterion) {
    let mut group = c.benchmark_group("router");
    
    // Create router with N routes / åˆ›å»ºæœ‰Nä¸ªè·¯ç”±çš„è·¯ç”±å™¨
    for route_count in [10, 100, 1000, 10000] {
        let mut router = Router::new();
        for i in 0..route_count {
            router = router.get(&format!("/route{}", i), || async { "ok" });
        }
        
        group.bench_with_input(
            BenchmarkId::new("match", route_count),
            &route_count,
            |b, _| {
                b.iter(|| {
                    // Match route / åŒ¹é…è·¯ç”±
                    black_box(router.match_route("/route999"));
                });
            },
        );
    }
    
    group.finish();
}

criterion_group!(benches, bench_router_match);
criterion_main!(benches);
```

---

## 5. Tools & Environment / å·¥å…·ä¸ç¯å¢ƒ

### 5.1 Benchmark Tools / åŸºå‡†æµ‹è¯•å·¥å…·

| Tool / å·¥å…· | Purpose / ç”¨é€” | Install / å®‰è£… |
|------------|---------------|---------------|
| **Criterion** | Micro benchmarks / å¾®åŸºå‡†æµ‹è¯• | `cargo add --dev criterion` |
| **wrk** | HTTP load testing / HTTPè´Ÿè½½æµ‹è¯• | `brew install wrk` (macOS) |
| **hey** | HTTP load generator / HTTPè´Ÿè½½ç”Ÿæˆå™¨ | `go install github.com/rakyll/hey@latest` |
| **oha** | Modern HTTP load tester / ç°ä»£HTTPè´Ÿè½½æµ‹è¯•å™¨ | `cargo install oha` |
| **hyperfine** | Command-line benchmarking / å‘½ä»¤è¡ŒåŸºå‡†æµ‹è¯• | `cargo install hyperfine` |
| **perf** (Linux) | CPU profiling / CPUæ€§èƒ½åˆ†æ | `apt install linux-tools-generic` |
| **Instruments** (macOS) | CPU/memory profiling / CPU/å†…å­˜åˆ†æ | Built-in / å†…ç½® |

### 5.2 Test Environment / æµ‹è¯•ç¯å¢ƒ

#### 5.2.1 Recommended Hardware / æ¨èç¡¬ä»¶

```yaml
Minimum Specification / æœ€ä½è§„æ ¼:
  CPU: 4 cores / 4æ ¸å¿ƒ
  RAM: 16GB
  Storage: SSD
  Network: 1Gbps

Recommended Specification / æ¨èè§„æ ¼:
  CPU: 8+ cores (Intel Xeon or AMD EPYC) / 8+æ ¸å¿ƒ
  RAM: 32GB+
  Storage: NVMe SSD
  Network: 10Gbps

TechEmpower Official / TechEmpowerå®˜æ–¹:
  CPU: Dell R440 - Intel Xeon Gold 5120 (14 cores)
  RAM: 32GB ECC
  Storage: SSD
  Network: 10Gbps dedicated
```

#### 5.2.2 OS Configuration / æ“ä½œç³»ç»Ÿé…ç½®

```bash
# Linux kernel tuning / Linuxå†…æ ¸è°ƒä¼˜
sudo sysctl -w net.core.somaxconn=65535
sudo sysctl -w net.ipv4.tcp_max_syn_backlog=8192
sudo sysctl -w net.ipv4.ip_local_port_range="1024 65535"
sudo sysctl -w net.core.netdev_max_backlog=65535
sudo sysctl -w fs.file-max=2097152

# Increase file descriptor limits / å¢åŠ æ–‡ä»¶æè¿°ç¬¦é™åˆ¶
ulimit -n 65535

# Disable CPU frequency scaling / ç¦ç”¨CPUé¢‘ç‡ç¼©æ”¾
sudo cpupower frequency-set --governor performance

# Enable io-uring (Linux 5.1+) / å¯ç”¨io-uring
# Verify: cat /proc/sys/kernel/io_uring_disabled (should be 0)
```

### 5.3 Profiling Tools / æ€§èƒ½åˆ†æå·¥å…·

#### 5.3.1 CPU Profiling / CPUæ€§èƒ½åˆ†æ

```bash
# Linux perf / Linuxæ€§èƒ½åˆ†æ
perf record --call-graph=dwarf ./target/release/nexus-example
perf report

# Flamegraph generation / ç«ç„°å›¾ç”Ÿæˆ
cargo install flamegraph
cargo flamegraph --bin nexus-example

# macOS Instruments / macOSæ€§èƒ½åˆ†æ
instruments -t "Time Profiler" ./target/release/nexus-example
```

#### 5.3.2 Memory Profiling / å†…å­˜åˆ†æ

```bash
# Valgrind (Linux) / Valgrindå†…å­˜åˆ†æ
valgrind --tool=massif ./target/release/nexus-example
massif-visualizer massif.out.*

# Heaptrack (Linux) / Heaptrackå†…å­˜è¿½è¸ª
heaptrack ./target/release/nexus-example
heaptrack_gui heaptrack.nexus-example.*
```

---

## 6. Running Benchmarks / è¿è¡ŒåŸºå‡†æµ‹è¯•

### 6.1 Micro Benchmarks / å¾®åŸºå‡†æµ‹è¯•

```bash
# Run all benchmarks / è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•
cargo bench

# Run specific benchmark / è¿è¡Œç‰¹å®šåŸºå‡†æµ‹è¯•
cargo bench --bench runtime_bench

# Save baseline for comparison / ä¿å­˜åŸºçº¿ç”¨äºæ¯”è¾ƒ
cargo bench --bench runtime_bench -- --save-baseline before-optimization

# Compare with baseline / ä¸åŸºçº¿æ¯”è¾ƒ
cargo bench --bench runtime_bench -- --baseline before-optimization

# Generate HTML reports / ç”ŸæˆHTMLæŠ¥å‘Š
cargo bench
open target/criterion/report/index.html
```

### 6.2 HTTP Load Testing / HTTPè´Ÿè½½æµ‹è¯•

#### 6.2.1 Using wrk / ä½¿ç”¨wrk

```bash
# Simple GET request / ç®€å•GETè¯·æ±‚
wrk -t4 -c100 -d30s http://127.0.0.1:3000/

# With script (POST JSON) / ä½¿ç”¨è„šæœ¬ï¼ˆPOST JSONï¼‰
wrk -t4 -c100 -d30s -s scripts/post.lua http://127.0.0.1:3000/api/users

# Latency percentiles / å»¶è¿Ÿç™¾åˆ†ä½
wrk -t4 -c100 -d30s --latency http://127.0.0.1:3000/
```

#### 6.2.2 Using hey / ä½¿ç”¨hey

```bash
# 10K requests, 100 concurrent / 10Kè¯·æ±‚ï¼Œ100å¹¶å‘
hey -n 10000 -c 100 http://127.0.0.1:3000/

# POST with JSON body / POSTå¸¦JSONä½“
hey -n 10000 -c 100 -m POST -H "Content-Type: application/json" \
    -d '{"name":"test"}' http://127.0.0.1:3000/api/users

# Save results / ä¿å­˜ç»“æœ
hey -n 10000 -c 100 -o csv http://127.0.0.1:3000/ > results.csv
```

#### 6.2.3 Using oha / ä½¿ç”¨oha

```bash
# Modern output with histogram / ç°ä»£åŒ–è¾“å‡ºå¸¦ç›´æ–¹å›¾
oha -n 10000 -c 100 http://127.0.0.1:3000/

# HTTP/2 testing / HTTP/2æµ‹è¯•
oha -n 10000 -c 100 --http2 https://127.0.0.1:3443/

# Save JSON results / ä¿å­˜JSONç»“æœ
oha -n 10000 -c 100 --json http://127.0.0.1:3000/ > results.json
```

### 6.3 Comparative Benchmarks / å¯¹æ¯”åŸºå‡†æµ‹è¯•

```bash
# Create test servers for comparison / åˆ›å»ºç”¨äºæ¯”è¾ƒçš„æµ‹è¯•æœåŠ¡å™¨

# 1. Nexus server / NexusæœåŠ¡å™¨
cargo run --release --bin nexus-echo-server &
NEXUS_PID=$!

# 2. Actix server / ActixæœåŠ¡å™¨
cargo run --release --bin actix-echo-server &
ACTIX_PID=$!

# Run benchmarks / è¿è¡ŒåŸºå‡†æµ‹è¯•
echo "Benchmarking Nexus..."
wrk -t4 -c100 -d30s http://127.0.0.1:3000/ > nexus_results.txt

echo "Benchmarking Actix..."
wrk -t4 -c100 -d30s http://127.0.0.1:3001/ > actix_results.txt

# Cleanup / æ¸…ç†
kill $NEXUS_PID $ACTIX_PID

# Compare results / æ¯”è¾ƒç»“æœ
python scripts/compare_bench.py nexus_results.txt actix_results.txt
```

---

## 7. Performance Regression Detection / æ€§èƒ½å›å½’æ£€æµ‹

### 7.1 CI/CD Integration / CI/CDé›†æˆ

```yaml
# .github/workflows/bench.yml
name: Benchmark

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  benchmark:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Install Rust / å®‰è£…Rust
        uses: actions-rs/toolchain@v1
        with:
          profile: minimal
          toolchain: stable
      
      - name: Run benchmarks / è¿è¡ŒåŸºå‡†æµ‹è¯•
        run: cargo bench --workspace -- --save-baseline current
      
      - name: Compare with main / ä¸mainåˆ†æ”¯æ¯”è¾ƒ
        if: github.event_name == 'pull_request'
        run: |
          git fetch origin main
          git checkout origin/main
          cargo bench --workspace -- --save-baseline main
          git checkout -
          cargo bench --workspace -- --baseline main
      
      - name: Upload results / ä¸Šä¼ ç»“æœ
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: target/criterion/
```

### 7.2 Regression Thresholds / å›å½’é˜ˆå€¼

```rust
// benches/support/regression.rs

/// Acceptable performance degradation / å¯æ¥å—çš„æ€§èƒ½ä¸‹é™
const MAX_REGRESSION: f64 = 0.05; // 5%

/// Detect regression from baseline / ä»åŸºçº¿æ£€æµ‹å›å½’
pub fn check_regression(baseline: Duration, current: Duration) -> Result<(), String> {
    let baseline_ns = baseline.as_nanos() as f64;
    let current_ns = current.as_nanos() as f64;
    
    let regression = (current_ns - baseline_ns) / baseline_ns;
    
    if regression > MAX_REGRESSION {
        Err(format!(
            "Performance regression detected: {:.1}% slower (max: {:.1}%)",
            regression * 100.0,
            MAX_REGRESSION * 100.0
        ))
    } else {
        Ok(())
    }
}
```

---

## 8. Benchmark Results Archive / åŸºå‡†æµ‹è¯•ç»“æœå½’æ¡£

### 8.1 Phase 1 Results (Pending) / ç¬¬1é˜¶æ®µç»“æœï¼ˆå¾…è¡¥å……ï¼‰

> **Note** / **æ³¨æ„**: Comprehensive benchmark results will be added once Phase 2 HTTP server is complete.
> **æ³¨æ„**ï¼šä¸€æ—¦ç¬¬2é˜¶æ®µHTTPæœåŠ¡å™¨å®Œæˆï¼Œå°†æ·»åŠ å…¨é¢çš„åŸºå‡†æµ‹è¯•ç»“æœã€‚

```
Phase 1 Runtime Benchmarks / ç¬¬1é˜¶æ®µè¿è¡Œæ—¶åŸºå‡†æµ‹è¯•
Expected completion: Phase 2 end (Month 7)
é¢„è®¡å®Œæˆæ—¶é—´ï¼šç¬¬2é˜¶æ®µç»“æŸï¼ˆç¬¬7ä¸ªæœˆï¼‰

Planned tests / è®¡åˆ’çš„æµ‹è¯•:
- Task spawn latency: < 10Âµs
- Channel throughput: > 10M ops/sec
- Timer operations: O(1), < 1Âµs
- TCP echo: > 1M req/sec
```

### 8.2 Historical Data / å†å²æ•°æ®

```
docs/benchmarks/
â”œâ”€â”€ phase1-runtime.md          # Phase 1 runtime results / ç¬¬1é˜¶æ®µè¿è¡Œæ—¶ç»“æœ
â”œâ”€â”€ phase2-http.md             # Phase 2 HTTP results / ç¬¬2é˜¶æ®µHTTPç»“æœ
â”œâ”€â”€ phase3-middleware.md       # Phase 3 middleware results / ç¬¬3é˜¶æ®µä¸­é—´ä»¶ç»“æœ
â””â”€â”€ comparisons/               # vs Tokio/Actix/Axum
    â”œâ”€â”€ vs-tokio.md
    â”œâ”€â”€ vs-actix.md
    â””â”€â”€ vs-axum.md
```

---

## 9. Best Practices / æœ€ä½³å®è·µ

### 9.1 Benchmark Design / åŸºå‡†æµ‹è¯•è®¾è®¡

âœ… **DO** / **åº”è¯¥åš**:
- Use `black_box()` to prevent compiler optimizations / ä½¿ç”¨`black_box()`é˜²æ­¢ç¼–è¯‘å™¨ä¼˜åŒ–
- Run warm-up iterations / è¿è¡Œé¢„çƒ­è¿­ä»£
- Test with realistic data sizes / ä½¿ç”¨çœŸå®æ•°æ®å¤§å°æµ‹è¯•
- Measure multiple metrics (throughput, latency, memory) / æµ‹é‡å¤šä¸ªæŒ‡æ ‡ï¼ˆååé‡ã€å»¶è¿Ÿã€å†…å­˜ï¼‰
- Document test environment / è®°å½•æµ‹è¯•ç¯å¢ƒ

âŒ **DON'T** / **ä¸åº”è¯¥åš**:
- Benchmark in debug mode / åœ¨è°ƒè¯•æ¨¡å¼ä¸‹åŸºå‡†æµ‹è¯•
- Ignore cold start effects / å¿½ç•¥å†·å¯åŠ¨æ•ˆåº”
- Cherry-pick best results / æŒ‘é€‰æœ€å¥½çš„ç»“æœ
- Run on battery power (laptops) / ä½¿ç”¨ç”µæ± ä¾›ç”µè¿è¡Œï¼ˆç¬”è®°æœ¬ç”µè„‘ï¼‰
- Ignore variance / å¿½ç•¥æ–¹å·®

### 9.2 Interpreting Results / è§£é‡Šç»“æœ

```
Example Criterion output / Criterionè¾“å‡ºç¤ºä¾‹:

spawn_latency           time:   [8.234 Âµs 8.456 Âµs 8.701 Âµs]
                        change: [-2.3% -0.8% +0.7%] (p = 0.18 > 0.05)
                        No change in performance detected.
                        æœªæ£€æµ‹åˆ°æ€§èƒ½å˜åŒ–ã€‚

Key metrics / å…³é”®æŒ‡æ ‡:
- time: [lower_bound estimate upper_bound] / æ—¶é—´ï¼š[ä¸‹ç•Œ ä¼°è®¡å€¼ ä¸Šç•Œ]
- change: Performance change from baseline / ä¸åŸºçº¿çš„æ€§èƒ½å˜åŒ–
- p-value: Statistical significance / ç»Ÿè®¡æ˜¾è‘—æ€§
```

### 9.3 Optimization Workflow / ä¼˜åŒ–å·¥ä½œæµ

```
1. Measure / æµ‹é‡
   â†“
2. Profile (find bottleneck) / åˆ†æï¼ˆæ‰¾åˆ°ç“¶é¢ˆï¼‰
   â†“
3. Optimize (fix bottleneck) / ä¼˜åŒ–ï¼ˆä¿®å¤ç“¶é¢ˆï¼‰
   â†“
4. Measure again (verify improvement) / å†æ¬¡æµ‹é‡ï¼ˆéªŒè¯æ”¹è¿›ï¼‰
   â†“
5. Repeat / é‡å¤
```

---

## 10. Troubleshooting / æ•…éšœæ’æŸ¥

### 10.1 Common Issues / å¸¸è§é—®é¢˜

**Issue** / **é—®é¢˜**: High variance in results / ç»“æœæ–¹å·®å¤§

**Solution** / **è§£å†³æ–¹æ¡ˆ**:
```bash
# Disable CPU frequency scaling / ç¦ç”¨CPUé¢‘ç‡ç¼©æ”¾
sudo cpupower frequency-set --governor performance

# Disable turbo boost / ç¦ç”¨ç¿é¢‘
echo 1 | sudo tee /sys/devices/system/cpu/intel_pstate/no_turbo

# Pin to specific cores / å›ºå®šåˆ°ç‰¹å®šæ ¸å¿ƒ
taskset -c 0-3 cargo bench
```

**Issue** / **é—®é¢˜**: I/O bottleneck / I/Oç“¶é¢ˆ

**Solution** / **è§£å†³æ–¹æ¡ˆ**:
```bash
# Check I/O scheduler / æ£€æŸ¥I/Oè°ƒåº¦å™¨
cat /sys/block/sda/queue/scheduler

# Use none or mq-deadline for NVMe / NVMeä½¿ç”¨noneæˆ–mq-deadline
echo none | sudo tee /sys/block/nvme0n1/queue/scheduler
```

**Issue** / **é—®é¢˜**: Network saturation / ç½‘ç»œé¥±å’Œ

**Solution** / **è§£å†³æ–¹æ¡ˆ**:
```bash
# Use loopback (127.0.0.1) for benchmarks / åŸºå‡†æµ‹è¯•ä½¿ç”¨å›ç¯åœ°å€
# Verify no packet drops / éªŒè¯æ— ä¸¢åŒ…
netstat -s | grep -i drop
```

---

## Appendix A: Benchmark Scripts / é™„å½•Aï¼šåŸºå‡†æµ‹è¯•è„šæœ¬

### A.1 Full Benchmark Suite / å®Œæ•´åŸºå‡†æµ‹è¯•å¥—ä»¶

```bash
#!/bin/bash
# scripts/run_all_benchmarks.sh

set -e

echo "ğŸš€ Running Nexus Framework Benchmarks"
echo "======================================"

# 1. Micro benchmarks / å¾®åŸºå‡†æµ‹è¯•
echo "ğŸ“Š Phase 1: Micro benchmarks..."
cargo bench --bench runtime_bench
cargo bench --bench scheduler_bench
cargo bench --bench channel_bench
cargo bench --bench timer_bench

# 2. HTTP benchmarks (Phase 2) / HTTPåŸºå‡†æµ‹è¯•ï¼ˆç¬¬2é˜¶æ®µï¼‰
echo "ğŸ“Š Phase 2: HTTP benchmarks..."
# TODO: Add HTTP benchmarks

# 3. Generate report / ç”ŸæˆæŠ¥å‘Š
echo "ğŸ“„ Generating report..."
python scripts/generate_report.py

echo "âœ… Benchmarks complete!"
echo "View results: open target/criterion/report/index.html"
```

---

## Appendix B: Reference Results / é™„å½•Bï¼šå‚è€ƒç»“æœ

### B.1 Target vs Actual (Template) / ç›®æ ‡vså®é™…ï¼ˆæ¨¡æ¿ï¼‰

```markdown
| Benchmark | Target | Actual | Status | Notes |
|-----------|--------|--------|--------|-------|
| Task spawn | < 10Âµs | TBD | ğŸ“Š | Phase 2 |
| Channel (bounded) | > 10M ops/s | TBD | ğŸ“Š | Phase 2 |
| Timer insert | < 1Âµs | TBD | ğŸ“Š | Phase 2 |
| TCP echo | > 1M req/s | TBD | ğŸ“Š | Phase 2 |
| HTTP parse | > 1GB/s | TBD | ğŸ“Š | Phase 2 |
| Router match | < 100ns | TBD | ğŸ“Š | Phase 2 |
```

---

**Last Updated** / **æœ€åæ›´æ–°**: 2026-01-24  
**Next Review** / **ä¸‹æ¬¡å®¡æŸ¥**: Phase 2 completion (Month 7)

---

For questions or contributions, see [CONTRIBUTING.md](../CONTRIBUTING.md).
